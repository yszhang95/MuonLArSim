#!/usr/bin/env python3

import pandas as pd
import json
import argparse
import os
import h5py
import numpy as np
import zipfile

def convert_hdf5_to_json(hdf5_file, output_prefix, event_ids=None):
    """
    Loads data from an HDF5 file (generated by convert.py), processes it,
    and saves results into separate JSON files based on EventID,
    structured as prefix/data/{event_id}/{event_id}-pgun.json.
    """
    try:
        with h5py.File(hdf5_file, 'r') as f:
            if 'segments' not in f:
                print(f"Error: 'segments' dataset not found in HDF5 file {hdf5_file}")
                return
            data = f['segments'][:]
    except FileNotFoundError:
        print(f"Error: HDF5 file not found at {hdf5_file}")
        return
    except Exception as e:
        print(f"Error reading HDF5 file {hdf5_file}: {e}")
        return

    # Reconstruct DataFrame structure based on convert.py's segment_dtype
    # We need specific fields from the structured array 'data'

    # Determine the dtype mapping for easier access (derived from convert.py)
    segment_dtype_fields = [
        ('t0_start', np.float64), ('t0_end', np.float64), ('t0', np.float64),
        ('t_start', np.float64), ('t_end', np.float64), ('t', np.float64),
        ('vertex_id', np.uint64),
        ('event_id', np.uint32), ('segment_id', np.uint32), ('traj_id', np.uint32),
        ('file_traj_id', np.uint32), ('n_electrons', np.uint32), ('pdg_id', np.int32),
        ('pixel_plane', np.int32),
        ('x_end', np.float32), ('y_end', np.float32), ('z_end', np.float32),
        ('x_start', np.float32), ('y_start', np.float32), ('z_start', np.float32),
        ('dx', np.float32), ('tran_diff', np.float32), ('long_diff', np.float32),
        ('dEdx', np.float32), ('dE', np.float32), ('n_photons', np.float32),
        ('x', np.float32), ('y', np.float32), ('z', np.float32),
    ]

    # Create a temporary Pandas DataFrame from the structured numpy array
    df_dict = {name: data[name] for name, _ in segment_dtype_fields}

    # Convert event_id to be 0-indexed if event_ids parameter was passed,
    # assuming the structure expects 0-indexed events internally for filtering logic
    # However, since we are only reading, we rely on 'event_id' field from HDF5.
    df = pd.DataFrame(df_dict)

    # The conversion logic in convert.py filters based on PDG_ID = |13| or |11|.
    # We must re-apply this filter here, as the HDF5 file *might* contain other particles
    # if it was generated without the specific muon/electron filter active, or if the user
    # expects the JSON output to only contain these if the HDF5 was created broadly.
    # Since convert.py filtered *before* writing to HDF5, we assume data here only
    # contains muons/electrons OR we must filter based on the original assumption.

    # Assuming the HDF5 was created only from Muons/Electrons steps as per convert.py:
    # If event_ids is provided, it implies a specific filtering/mapping was done in convert.py
    # that might affect how 'event_id' is interpreted if it wasn't the raw EventID.

    if event_ids is not None:
        # If event_ids was supplied to convert.py (meaning it mapped internal IDs to actual IDs),
        # the 'event_id' column in HDF5 might be sequential 0, 1, 2... corresponding to event_ids[i].
        # This path is complex without seeing the calling context from convert.py main().
        # For simplicity and matching the JSON output structure, we assume 'event_id' in HDF5
        # corresponds directly to the identifier used for directory naming, which was the original EventID
        # or the mapped ID if eventid_as_runid logic was involved in *writing* to HDF5.

        # If event_ids is passed here, we must filter the dataframe based on which event_id
        # it corresponds to, but since we don't know the mapping logic from the HDF5 generation call,
        # we rely solely on the 'event_id' field present in the data if possible.

        # Fallback: If we are using event_ids, we assume we only want events present in that list.
        # This might be too strict if event_ids was intended only for run ID mapping in the HDF5 context.
        df = df[df['event_id'].isin(event_ids)]

    # Since the HDF5 source logic in convert.py seems complex regarding event_id mapping,
    # we rely on the 'event_id' field already present in the data, which should represent
    # the EventID used for grouping in the original script.

    if df.empty:
        print(f"No relevant event data found in {hdf5_file}.")
        return

    # Calculate center coordinates (x, y, z) - these fields ('x', 'y', 'z') are already calculated
    # and present in the HDF5 structure from convert.py. We use them directly.

    # Group by EventID to create one JSON object per event
    grouped = df.groupby('event_id')

    # Static requirements from the original implementation
    run_no = "0"
    sub_run_no = "0"
    geom = "2x2"
    type_str = "particle-gun" # This matches the output filename convention
    cluster_id_val = 1
    real_cluster_id_val = 1

    for event_id, group in grouped:
        # Prepare the JSON structure for this event
        event_json = {
            "runNo": run_no,
            "subRunNo": sub_run_no,
            "eventNo": str(event_id),
            # Use pre-calculated 'x', 'y', 'z' from HDF5 structure
            "x": group['x'].tolist(),
            "y": group['y'].tolist(),
            "z": group['z'].tolist(),
            "cluster_id": [cluster_id_val] * len(group),
            "real_cluster_id": [real_cluster_id_val] * len(group),
            "geom": geom,
            "type": type_str,
        }

        # Determine output path: prefix/data/{event_id}/{event_id}-pgun.json
        event_dir = os.path.join(output_prefix, "data", str(event_id))
        os.makedirs(event_dir, exist_ok=True)
        json_file = os.path.join(event_dir, f"{event_id}-pgun.json")

        # Save event data into the output JSON file
        try:
            with open(json_file, 'w') as f:
                # Saving a single event dictionary
                json.dump(event_json, f, indent=4)
            print(f"Successfully wrote event {event_id} to {json_file}")
        except Exception as e:
            print(f"Error writing JSON file {json_file}: {e}")


def zip_output_directory(output_prefix):
    """
    Compresses all files recursively within output_prefix/data/ into
    output_prefix/data.zip.
    """
    data_dir = os.path.join(output_prefix, "data")
    zip_file_path = os.path.join(output_prefix, f"{output_prefix}.zip")

    if not os.path.isdir(data_dir):
        print(f"Error: Data directory not found at {data_dir}. Skipping zip.")
        return

    print(f"Creating zip archive at {zip_file_path} from contents of {data_dir}...")

    try:
        with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            for root, _, files in os.walk(data_dir):
                for file in files:
                    full_path = os.path.join(root, file)
                    # Create arcname relative to output_prefix/data/
                    arcname = os.path.relpath(full_path, start=os.path.dirname(data_dir))
                    zf.write(full_path, arcname)
        print(f"Successfully created zip file: {zip_file_path}")
    except Exception as e:
        print(f"Error creating zip file {zip_file_path}: {e}")


def main():
    parser = argparse.ArgumentParser(description="Convert simulation step HDF5 to JSON format, one file per event.")
    parser.add_argument("hdf5_file", help="Path to the input HDF5 file (from convert.py).")
    parser.add_argument("output_prefix", help="Path to the output prefix directory (e.g., /path/to/output).")
    parser.add_argument("--compress", action="store_true", help="Compress the output 'data' directory into a zip file after conversion.")

    args = parser.parse_args()


    convert_hdf5_to_json(args.hdf5_file, args.output_prefix, None)

    if args.compress:
        zip_output_directory(args.output_prefix)


if __name__ == "__main__":
    main()
